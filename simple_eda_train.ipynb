{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_eda_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv7IqJFxr2F8"
      },
      "source": [
        "'''\n",
        "Author    : Ramakrishnan Radhakrishnan\n",
        "Goal      : Simple EDA and Model training for MLops and then increase complexity \n",
        "            gradually as the production deployment versions increase.\n",
        "References: Much of the work are either inspired or referred from the following.\n",
        "Along with official tensor flow docs,\n",
        "1) https://builtin.com/data-science/guide-logistic-regression-tensorflow-20\n",
        "2) https://towardsdatascience.com/natural-language-processing-with-tensorflow-e0a701ef5cef?gi=33ea4b1c117c\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpwmR_sIWmQS",
        "outputId": "6170813c-f1be-4f05-c06e-ca329865c352"
      },
      "source": [
        "'''\n",
        "Cloning the project to get source data.\n",
        "In case of confidential data, it is better to upload from local / read from \n",
        "disk to work on it.\n",
        "'''\n",
        "!git clone https://github.com/Ramakrishnanr/wake_word_detection.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'wake_word_detection'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 18 (delta 4), reused 9 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (18/18), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaKUw6EpkGck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1daced4-1167-48cc-9b21-64a9e22b4a9d"
      },
      "source": [
        "cd wake_word_detection/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/wake_word_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbVrMwAQjcBD",
        "outputId": "214d73ed-6482-43a4-ad24-bb887c007485"
      },
      "source": [
        "## data_setup branch is for data related analysis.\n",
        "!git checkout data_setup"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Branch 'data_setup' set up to track remote branch 'data_setup' from 'origin'.\n",
            "Switched to a new branch 'data_setup'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POjQPxkFkscw"
      },
      "source": [
        "# Data handling related imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# ML domain related imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx3nOGwUksdT"
      },
      "source": [
        "class DataHandle():\n",
        "  def __init__(self, path = None, batch_size = None, random_seed = None):\n",
        "    if path is None:\n",
        "      self.data_path = '/content/wake_word_detection/data/train.csv'\n",
        "    else:\n",
        "      self.data_path = path\n",
        "    \n",
        "    ''' \n",
        "    In future, for huge dataset batch size will play a significant role \n",
        "    unlike now.\n",
        "    '''\n",
        "    if batch_size is None:\n",
        "      self.batch_size = 10\n",
        "    else:\n",
        "      self.batch_size = batch_size\n",
        "    \n",
        "    if random_seed is None:\n",
        "      self.random_seed = 5000\n",
        "    else:\n",
        "      self.random_seed = random_seed\n",
        "\n",
        "    self.utterances = 'word'\n",
        "    self.labels = 'label'\n",
        "    self.test_size = 0.20\n",
        "    self.random_state = 101\n",
        "\n",
        "    self.prefetch = 1\n",
        "\n",
        "  def read_data(self):\n",
        "    return pd.read_csv(self.data_path)\n",
        "\n",
        "  def check_prior(self, df):\n",
        "    label_series = df.label.value_counts()\n",
        "    wake_count_normalised      = (label_series[0] / (label_series[0] + \n",
        "                                                     label_series[1]))\n",
        "    non_wake_count_normalised  = (label_series[1] / (label_series[0] + \n",
        "                                                     label_series[1]))\n",
        "    return wake_count_normalised, non_wake_count_normalised\n",
        "\n",
        "  def disp_prior(self):\n",
        "    print(\"*****Prior Distribution*****\")\n",
        "    print(\"Wake Word:\", wake_normalised)\n",
        "    print(\"Non Wake Word:\", non_wake_normalised)\n",
        "\n",
        "  def get_train_val(self, X, Y):\n",
        "    return train_test_split(X, Y, test_size= self.test_size, \n",
        "                            random_state= self.random_state)\n",
        "\n",
        "  def parallelize_train_data(self, words_to_int, labels):\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((words_to_int, labels))\n",
        "    train_data = train_data.repeat().shuffle(self.random_seed).batch(self.batch_size).prefetch(self.prefetch)\n",
        "    return train_data\n",
        "\n",
        "  def get_X(self, df):\n",
        "    X = tf.data.Dataset.from_tensor_slices(df['self.utterances'])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLUFiqT_SEaW"
      },
      "source": [
        "class Preprocess():\n",
        "   def __init__(self, max_features = None, seq_length = None):\n",
        "     if max_features is None:\n",
        "       self.max_features = 100\n",
        "     else:\n",
        "       self.max_features = max_features\n",
        "\n",
        "     if seq_length is None:\n",
        "      self.seq_length = 25 ## For wake word, we don't need bigger. Can extend. \n",
        "     else:\n",
        "      self.seq_length = seq_length\n",
        "\n",
        "   def tokenize_utterances(self, x):\n",
        "    tokenizer = Tokenizer(num_words = self.max_features, oov_token = \"<OOV>\")\n",
        "    tokenizer.fit_on_texts(x)\n",
        "    word_index = tokenizer.word_index\n",
        "    sequences = tokenizer.texts_to_sequences(x)\n",
        "    padded_seq = pad_sequences(sequences, padding = 'post', truncating='post')\n",
        "    padded_seq = padded_seq.astype('float32')\n",
        "    return word_index, padded_seq"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q7IHlzBXyF2"
      },
      "source": [
        "class Models():\n",
        "  def __init__(self, no_of_classes = None, no_of_features = None, \n",
        "               learn_rate = None, train_steps = None, disp_step = None,\n",
        "               weights_path = None, bias_path = None):\n",
        "    if no_of_classes is None:\n",
        "      self.no_of_classes = 2\n",
        "    else:\n",
        "      self.no_of_classes = no_of_classes\n",
        "    \n",
        "    if no_of_features is None:\n",
        "      self.no_of_features = 1\n",
        "    else:\n",
        "      self.no_of_features = no_of_features\n",
        "    \n",
        "    if learn_rate is None:\n",
        "      self.learn_rate = 0.01\n",
        "    else:\n",
        "      self.learn_rate = learn_rate\n",
        "\n",
        "    if train_steps is None:\n",
        "      self.train_steps = 10\n",
        "    else:\n",
        "      self.train_steps = train_steps\n",
        "\n",
        "    if disp_step is None:\n",
        "      self.disp_step = 1\n",
        "    else:\n",
        "      self.disp_step = disp_step\n",
        "    self.weights_file = 'weights.txt'\n",
        "    self.bias_file = 'bias.txt'\n",
        "    self.word_index = 'word_index.txt'\n",
        "    \n",
        "  def get_weights(self, words_to_int):\n",
        "    print(np.shape(words_to_int)[1])\n",
        "    return tf.Variable(tf.ones([(np.shape(words_to_int)[1]), self.no_of_classes])\n",
        "    , name=\"weights\")\n",
        "    \n",
        "  def get_bias(self):\n",
        "    return tf.Variable(tf.zeros([self.no_of_classes]), name=\"bias\")\n",
        "\n",
        "  def log_regression(self, x, W, b):\n",
        "    return tf.nn.softmax(tf.matmul(x, W) + b)\n",
        "\n",
        "  def get_accuracy(self, y_pred, y_true):\n",
        "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
        "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "  def get_optimizer(self):\n",
        "    # Scope of improvement: With Adam optimizers in future.\n",
        "    return tf.optimizers.SGD(self.learn_rate)\n",
        "\n",
        "  def run_optimization(self, x, y, W, b, optimizer):\n",
        "    evaluation = Evaluation()\n",
        "    with tf.GradientTape() as g:\n",
        "        pred = self.log_regression(x, W, b)\n",
        "        loss = evaluation.loss_fn(pred, y)\n",
        "\n",
        "    gradients = g.gradient(loss, [W, b])\n",
        "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
        "\n",
        "  def run_training(self, train_data, W, b, optimizer):\n",
        "    evaluation = Evaluation()\n",
        "    for step, (batch_x, batch_y) in enumerate(train_data.take(self.train_steps)\n",
        "    , 1):\n",
        "      self.run_optimization(batch_x, batch_y, W, b, optimizer)\n",
        "      if step % self.disp_step == 0:\n",
        "        pred = self.log_regression(batch_x, W, b)\n",
        "        loss = evaluation.loss_fn(pred, batch_y)\n",
        "        acc = self.get_accuracy(pred, batch_y)\n",
        "        print(\"Step: %i, Loss: %f, Accuracy: %f\" % (step, loss, acc))\n",
        "    return W, b\n",
        "\n",
        "  def save_params(self, W, b, word_index):\n",
        "    # For logistic regression, we use simple saving. \n",
        "    # For NN, we can use tf.train.checkpoint / tf.keras.Model.save_weights. \n",
        "    #np.savetxt(self.weights, W)\n",
        "    #np.savetxt(self.bias, b)\n",
        "    weights_file = open(self.weights_file, 'wb')\n",
        "    pickle.dump(W, weights_file)\n",
        "    \n",
        "    bias_file = open(self.bias_file, 'wb')\n",
        "    pickle.dump(b, bias_file)\n",
        "\n",
        "    #np.savetxt(self.word_index, word_index) # To match the vocab in test data.\n",
        "    print(\"Model params saved successfully\")"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHcRkP1PwpZD"
      },
      "source": [
        "class Evaluation():\n",
        "  def __init__(self, loss_metric = None, no_of_classes = None):\n",
        "    if loss_metric is None:\n",
        "      self.loss_metric = 'cross entropy'\n",
        "    else:\n",
        "      self.loss_metric = loss_metric\n",
        "    if no_of_classes is None:\n",
        "      self.no_of_classes = 2\n",
        "    else:\n",
        "      self.no_of_classes = no_of_classes\n",
        "\n",
        "  def loss_fn(self, y_pred, y_actual):\n",
        "    if self.loss_metric == 'cross entropy':\n",
        "      y_actual = tf.one_hot(y_actual, depth=self.no_of_classes)\n",
        "      # To avoid log(0) error,\n",
        "      y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
        "      return tf.reduce_mean(-tf.reduce_sum(y_actual * tf.math.log(y_pred)))\n",
        "    else:\n",
        "      print(\"Currently only cross entropy is being provided.\")\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI9Ob9sCk-O7",
        "outputId": "5d1e99bd-bcd8-46d9-f995-5f550639fc1a"
      },
      "source": [
        "## EDA \n",
        "data_handle = DataHandle()\n",
        "source_df = data_handle.read_data()\n",
        "wake_normalised, non_wake_normalised = data_handle.check_prior(source_df)\n",
        "data_handle.disp_prior()\n",
        "\n",
        "## Preprocessing\n",
        "preprocess = Preprocess()\n",
        "word_index, X = preprocess.tokenize_utterances(source_df['word'])\n",
        "Y = source_df['label'].to_numpy()\n",
        "\n",
        "## Data split\n",
        "x_train, x_val, y_train, y_val = data_handle.get_train_val(X, Y)\n",
        "''' \n",
        "Please note: x_train_prefetch's datastructure:\n",
        "tensorflow.python.data.ops.dataset_ops.PrefetchDataset \n",
        "'''\n",
        "x_train_prefetch = data_handle.parallelize_train_data(x_train, y_train)\n",
        "\n",
        "## Training\n",
        "models = Models()\n",
        "initial_weights = models.get_weights(x_train)\n",
        "initial_bias = models.get_bias()\n",
        "iterator = iter(x_train_prefetch)\n",
        "optimizer = models.get_optimizer()\n",
        "trained_weights, trained_bias = models.run_training(x_train_prefetch, \n",
        "                                                    initial_weights, \n",
        "                                                    initial_bias, optimizer)\n",
        "\n",
        "## Testing\n",
        "y_pred = models.log_regression(x_val, trained_weights, trained_bias)\n",
        "accuracy = models.get_accuracy(y_pred, y_val)\n",
        "print(\"Accuracy: %f\" % accuracy)\n",
        "\n",
        "## Save model params\n",
        "models.save_params(trained_weights, trained_bias, word_index)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*****Prior Distribution*****\n",
            "Wake Word: 0.64\n",
            "Non Wake Word: 0.36\n",
            "6\n",
            "Step: 1, Loss: 6.169082, Accuracy: 0.800000\n",
            "Step: 2, Loss: 7.975520, Accuracy: 0.700000\n",
            "Step: 3, Loss: 3.318807, Accuracy: 0.900000\n",
            "Step: 4, Loss: 32.283810, Accuracy: 0.500000\n",
            "Step: 5, Loss: 9.242602, Accuracy: 0.300000\n",
            "Step: 6, Loss: 8.915598, Accuracy: 0.300000\n",
            "Step: 7, Loss: 3.867907, Accuracy: 0.800000\n",
            "Step: 8, Loss: 5.102798, Accuracy: 0.800000\n",
            "Step: 9, Loss: 25.858589, Accuracy: 0.600000\n",
            "Step: 10, Loss: 9.622684, Accuracy: 0.800000\n",
            "Accuracy: 0.600000\n",
            "Model params saved successfully\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}